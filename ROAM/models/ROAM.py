import torch
from torch import nn
import torch.nn.functional as F
from torch import einsum 
from einops import rearrange, repeat
from position_embedding import positionalencoding2d
import h5py
import math
from timm.models.layers import trunc_normal_

'''
Function for reading patch features of each ROI generated by pretrained models like ResNet 50.
each ROI has 84 patches (64(20x)+16(10x)+4(5x)=84)
'''
def read_features(feat_path):
    with h5py.File(feat_path,'r') as hdf5_file:
        features = hdf5_file['features'][:] # num_rois,84,1024
    return torch.from_numpy(features)


'''
Basic Transformer encoder network, modified from ViT-pytorch: https://github.com/lucidrains/vit-pytorch
The encoder includes FeedForward module and self-Attention module.
'''

class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

'''
FeedForward module
args:
    dim: input feature dimensions
    hidden_dim: hidden layer dimensions
    dropout: dropout layer (default = 0, no dropout)
'''
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)


'''
Self-Attention module
args:
    dim: input feature dimensions
    heads: number of attention heads
    dim_head: feature dimensions of each head
    dropout: dropout layer (default = 0, no dropout)
'''
class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., **kwargs):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.dropout = nn.Dropout(dropout)


        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
        self.norm = nn.LayerNorm(dim_head)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)

        dots = torch.matmul(self.norm(q), self.norm(k).transpose(-1, -2)) * self.scale

        attn = self.attend(dots)
        attn = self.dropout(attn)

        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

'''
Modified Self-Attention module:
    add relative positional bias to self-attention: softmax(q*k+rel_pos)
    Refer to MaxViT
args:
    dim: input feature dimensions
    heads: number of attention heads
    dim_head: feature dimensions of each head
    dropout: dropout layer (default = 0, no dropout)
    have_cls_token: whether the module has class token, if false, use mean pooling for downstream task
    kwargs:
        window_size: Windos size for computing relative position bias. For instance,a ROI at 20x has 
            8*8=64 patches (each patch can be regarded as a pixel), window_size is 8.
        shared_pe: whether to share relative position embedding across all the heads
'''
class Rel_Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.,
                 have_cls_token=True, **kwargs):
        super().__init__()

        self.have_cls_token = have_cls_token
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        num_rel_position = 2*kwargs['window_size'] - 1 
       
        h_range = torch.arange(kwargs['window_size']).cuda()
        w_range = torch.arange(kwargs['window_size']).cuda()
        grid_x, grid_y = torch.meshgrid(h_range, w_range)
        grid = torch.stack((grid_x, grid_y))
        grid = rearrange(grid, 'c h w -> c (h w)')
        grid = (grid[:,:,None]-grid[:,None,:]) + (kwargs['window_size']-1)
        self.bias_indices = (grid * torch.tensor([1,
                                                  num_rel_position]).cuda()[:,None,None]).sum(dim=0)

        if kwargs['shared_pe'] == True:
            self.rel_pe = nn.Embedding(num_rel_position**2, 1)
        else: 
            self.rel_pe = nn.Embedding(num_rel_position**2, heads)

        trunc_normal_(self.rel_pe.weight, std=0.02)
        
        self.attend = nn.Softmax(dim = -1)
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
        self.norm = nn.LayerNorm(dim_head)
        self.dropout = nn.Dropout(dropout)

        
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):

        rel_position_bias = self.rel_pe(self.bias_indices)
        rel_position_bias = rearrange(rel_position_bias, 'i j h -> () h i j')

        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)

        dots = torch.matmul(self.norm(q), self.norm(k).transpose(-1, -2)) * self.scale
        #dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        if self.have_cls_token: 
            dots[:,:,1:,1:] += rel_position_bias
        else:
            dots += rel_position_bias

        attn = self.attend(dots)   
        attn = self.dropout(attn)

        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)


'''
Transformer block
args:
    dim: input feature dimension
    depth: the depth (number of Transformer layers) of the Transformer block
    heads: number of heads
    dim_head: feature dimension of each head
    mlp_dim: dimension of hidden layer feature in FeedForward module
    attn_type: the type of self-attention layer ('sa': normal self-attention, 'rel_sa':relative self-attention)
    shared_pe: whether to share relative position embedding across all the heads
    window_size: Windos size for computing relative position bias.
    have_cls_token: whether the module has class token, if false, use mean pooling for downstream task
'''
class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.,
                 attn_type='sa', shared_pe=None, window_size=None,
                 have_cls_token=True):
        super().__init__()
        self.layers = nn.ModuleList([])
        if attn_type == 'sa':
            attn_layer = Attention
        elif attn_type == 'rel_sa':
            attn_layer = Rel_Attention
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, attn_layer(dim, heads = heads, dim_head =dim_head, 
                                        dropout = dropout,
                                        shared_pe=shared_pe,
                                        window_size=window_size,
                                        have_cls_token=have_cls_token)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
            ]))
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x


'''
ViT model
args:
    num_patches: the number of patches (the number of tokens)
    patch_dim: dimension of input tokens
    pool: whether to use cls token. 'cls': use cls token. 'mean': no cls token, use mean poolling.
    position = whether the positinoal encoding is learnable. 'learnable' or 'fixed'
    other arguments are the same with Transformer
'''
class ViT(nn.Module):
    def __init__(self, num_patches, patch_dim, dim, depth, heads, mlp_dim, 
                 pool = 'cls', dim_head = 64, dropout = 0., emb_dropout = 0.,
                 position='learnabel'):
        super().__init__()

        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        self.patch_dim = patch_dim
        self.dim = dim
        self.to_patch_embedding = nn.Sequential(
            #nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            #nn.LayerNorm(dim)
        )
        if position=='learnable':
            self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        elif position == 'fixed':
            pos_emb_20 = positionalencoding2d(dim, 8, 8)
            pos_emb_10 = positionalencoding2d(dim, 4, 4, 2)[1:]
            pos_emb_5 = positionalencoding2d(dim, 2, 2, 4)[1:]
            self.pos_embedding = torch.cat([pos_emb_20, pos_emb_10,
                                            pos_emb_5]).unsqueeze(0).cuda()
            print(self.pos_embedding.shape)
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout = nn.Dropout(emb_dropout)

        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)

        self.pool = pool
        self.to_latent = nn.Identity()

    def forward(self, x):
        if self.patch_dim != self.dim:
            x = self.to_patch_embedding(x)
        b, n, _ = x.shape

        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)
        x = torch.cat((cls_tokens, x), dim=1)
        x += self.pos_embedding[:, :(n + 1)]
        x = self.dropout(x)

        x = self.transformer(x)

        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]

        x = self.to_latent(x)
        return x


'''
Different versions of multi-scale self-attention network:
    different size of input ROI:
        PyramidViT_0: ROI size is 2048 (at 20x magnification level) 
        PyramidViT_1: ROI size is 1024 (at 20x magnification level) 
        PyramidViT_2: ROI size is 512 (at 20x magnification level) 
    
    PyramidViT_SingleScale: singel-scale model

    PyramidViT_wo_interscale: there is no inter-scale self-attention modules
'''

'''
PyramidViT_0:
    Input ROI has size of 2048 (at 20x). The number of tokens (patches) at 20x is 64. The size of each patch is 256x256.
    args:
        embed_weights: weight coefficient of instance embedding at each magnificant level (20x,10x,5x)
            List with the length of 3 or 'None' (learnable weights).
        depths: depth of transformer block at each scale.
            List with the length of 5:
                [intra-scale SA at 20x, 
                inter-scale between 20x~10x,
                intra-scale SA at 10x, 
                inter-scale between 10x~5x, 
                intra-scale SA at 5x]
        ape: whether to use positional encoding
'''
class PyramidViT_0(nn.Module):
    def __init__(self, num_patches, embed_weights, patch_dim, dim, depths, heads=4,
                 mlp_dim=512, pool='cls', dim_head = 64, dropout = 0., emb_dropout = 0.,
                 ape=True, attn_type='rel_sa', shared_pe=True):
        super().__init__()

        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        self.pool = pool

        self.patch_dim = patch_dim #dim of features extracted from ResNet
        self.dim = dim
        self.ape = ape
        self.embed_weights = embed_weights
        self.to_patch_embedding_20 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            #nn.LayerNorm(dim)
        )
        self.to_patch_embedding_10 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            #nn.LayerNorm(dim)
        )
        self.to_patch_embedding_5 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            #nn.LayerNorm(dim)
        )

        if ape:
            addition = 1 if pool == 'cls' else 0        #### pos embedding for cls_token
            self.pos_emb_20 = nn.Parameter(torch.zeros(1, 64+addition, dim))
            trunc_normal_(self.pos_emb_20, std=0.02)
            self.pos_emb_10 = nn.Parameter(torch.zeros(1, 16+addition, dim))
            trunc_normal_(self.pos_emb_10, std=0.02)
            self.pos_emb_5 = nn.Parameter(torch.zeros(1, 4+addition, dim))
            trunc_normal_(self.pos_emb_5, std=0.02)

        have_cls_token = False
        if pool == 'cls':
            have_cls_token = True
            self.cls_token_20 = nn.Parameter(torch.randn(1, 1, dim))
            self.cls_token_10 = nn.Parameter(torch.randn(1, 1, dim))
            self.cls_token_5 = nn.Parameter(torch.randn(1, 1, dim))
            trunc_normal_(self.cls_token_20, std=0.02)
            trunc_normal_(self.cls_token_10, std=0.02)
            trunc_normal_(self.cls_token_5, std=0.02)


        self.dropout = nn.Dropout(emb_dropout)

        assert len(depths) == 5
        self.transformer_20 = Transformer(dim, depths[0], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, 8,
                                          have_cls_token)
        self.transformer_20_to_10 = Transformer(dim, depths[1], heads, dim_head,
                                                mlp_dim, dropout, 'sa')     # no need to set have_cls_token
        self.transformer_10 = Transformer(dim, depths[2], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, 4,
                                          have_cls_token)
        self.transformer_10_to_5 = Transformer(dim, depths[3], heads, dim_head,
                                               mlp_dim, dropout, 'sa')
        self.transformer_5 = Transformer(dim, depths[4], heads, dim_head, mlp_dim, dropout,
                                         attn_type, shared_pe, 2,
                                         have_cls_token)

        if embed_weights == None:
            print('learnable embedding weights')
            self.learned_weights = nn.Parameter(torch.Tensor(3,1))
            ## init
            nn.init.kaiming_uniform(self.learned_weights,a=math.sqrt(5))

        self.ms_dropout = nn.Dropout(dropout)



    def forward(self, x):
        b, _, _ = x.shape
        
        
        x_20 = x[:, :64, :]     # b 64 c
        x_10 = x[:, 64:80, :]   # b 16 c
        x_5 = x[:, 80:84, :]    # b 4 c
        
        if self.patch_dim != self.dim:
            x_20 = self.to_patch_embedding_20(x_20)
            x_10 = self.to_patch_embedding_10(x_10)
            x_5 = self.to_patch_embedding_5(x_5)

        if self.pool == 'cls':
            cls_token_20 = repeat(self.cls_token_20, '() n d -> b n d', b=b)  #b 1 c
            x_20 = torch.cat((cls_token_20, x_20), dim=1)   #b 65 c
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 65 c
            x_20_cls_token = x_20[:,0,:]      # b c
            x_20 = x_20[:,1:,:]     #b 64 c

            x_20 = rearrange(x_20, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=4, h2=2, w1=4, w2=2)    # 16b 4 c
            x_10 = rearrange(x_10, 'b (n m) c -> (b n) m c', m=1)  # 16b 1 c
            x_20_10 = torch.cat((x_10, x_20), dim=1)    # 16b 5 c
            x_20_10 = self.transformer_20_to_10(x_20_10) # 16b 5 c
            x_10 = x_20_10[:, 0:1, :]       # 16b 1 c

            cls_token_10 = repeat(self.cls_token_10, '() n d -> b n d', b=b)  #b 1 c
            x_10 = rearrange(x_10, '(b n) m c -> b (n m) c', b=b)   #b 16 c
            x_10 = torch.cat((cls_token_10, x_10), dim=1)   #b 17 c
            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 17 c
            x_10_cls_token = x_10[:,0,:]    # b c
            x_10 = x_10[:,1:,:]     # b 16 c

            x_10 = rearrange(x_10, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=2, h2=2, w1=2, w2=2)    # 4b 4 c
            x_5 = rearrange(x_5, 'b (n m) c -> (b n) m c', m=1)  # 4b 1 c
            x_10_5 = torch.cat((x_5, x_10), dim=1)    # 4b 5 c
            x_10_5 = self.transformer_10_to_5(x_10_5) # 4b 5 c
            x_5 = x_10_5[:, 0:1, :]       # 4b 1 c

            cls_token_5 = repeat(self.cls_token_5, '() n d -> b n d', b=b)  #b 1 c
            x_5 = rearrange(x_5, '(b n) m c -> b (n m) c', b=b)   #b 4 c
            x_5 = torch.cat((cls_token_5, x_5), dim=1)   #b 5 c
            if self.ape:
                x_5 += self.pos_emb_5
            x_5 = self.dropout(x_5)   
            x_5 = self.transformer_5(x_5)    # b 5 c
            x_5_cls_token = x_5[:,0,:]      # b c

        elif self.pool == 'mean':
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 64 c
            x_20_cls_token = x_20.mean(dim=1)      # b c

            x_20 = rearrange(x_20, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=4, h2=2, w1=4, w2=2)    # 16b 4 c
            x_10 = rearrange(x_10, 'b (n m) c -> (b n) m c', m=1)  # 16b 1 c
            x_20_10 = torch.cat((x_10, x_20), dim=1)    # 16b 5 c
            x_20_10 = self.transformer_20_to_10(x_20_10) # 16b 5 c
            x_10 = x_20_10[:, 0:1, :]       # 16b 1 c

            x_10 = rearrange(x_10, '(b n) m c -> b (n m) c', b=b)   #b 16 c
            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 16 c
            x_10_cls_token = x_10.mean(dim=1)      # b c

            x_10 = rearrange(x_10, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=2, h2=2, w1=2, w2=2)    # 4b 4 c
            x_5 = rearrange(x_5, 'b (n m) c -> (b n) m c', m=1)  # 4b 1 c
            x_10_5 = torch.cat((x_5, x_10), dim=1)    # 4b 5 c
            x_10_5 = self.transformer_10_to_5(x_10_5) # 4b 5 c
            x_5 = x_10_5[:, 0:1, :]       # 4b 1 c

            x_5 = rearrange(x_5, '(b n) m c -> b (n m) c', b=b)   #b 4 c
            if self.ape:
                x_5 += self.pos_emb_5
            x_5 = self.dropout(x_5)   
            x_5 = self.transformer_5(x_5)    # b 4 c
            x_5_cls_token = x_5.mean(dim=1)      # b c
        

        if self.embed_weights == None:
            learned_weights = torch.softmax(self.learned_weights,dim=0)

            x = learned_weights[0]*x_5_cls_token + learned_weights[1]*x_10_cls_token + learned_weights[2]*x_20_cls_token


       
        else:
            x_stack = torch.stack((self.embed_weights[0]*x_5_cls_token, 
                                   self.embed_weights[1]*x_10_cls_token, 
                                   self.embed_weights[2]*x_20_cls_token))
            x = torch.sum(x_stack,dim=0) # b c
        
        
        return x


'''
PyramidViT_1:
    Input ROI has size of 1024 (at 20x). The number of tokens (patches) at 20x is 16. The size of each patch is 256x256.
    args:
        embed_weights: weight coefficient of instance embedding at each magnificant level (20x,10x,5x)
            List with the length of 3 or 'None' (learnable weights).
        depths: depth of transformer block at each scale.
            List with the length of 5:
                [intra-scale SA at 20x, 
                inter-scale between 20x~10x,
                intra-scale SA at 10x, 
                inter-scale between 10x~5x, 
                intra-scale SA at 5x]
        ape: whether to use positional encoding
'''
class PyramidViT_1(nn.Module):
    def __init__(self, num_patches, embed_weights, patch_dim, dim, depths, heads=4,
                 mlp_dim=512, pool='cls', dim_head = 64, dropout = 0., emb_dropout = 0.,
                 ape=True, attn_type='rel_sa', shared_pe=True):
        super().__init__()

        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        self.pool = pool

        self.patch_dim = patch_dim #dim of features extracted from ResNet
        self.dim = dim
        self.ape = ape
        self.embed_weights = embed_weights
        self.to_patch_embedding_20 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )
        self.to_patch_embedding_10 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )
        self.to_patch_embedding_5 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )

        if ape:
            addition = 1 if pool == 'cls' else 0        #### pos embedding for cls_token
            self.pos_emb_20 = nn.Parameter(torch.zeros(1, 16+addition, dim))
            trunc_normal_(self.pos_emb_20, std=0.02)
            self.pos_emb_10 = nn.Parameter(torch.zeros(1, 4+addition, dim))
            trunc_normal_(self.pos_emb_10, std=0.02)
            self.pos_emb_5 = nn.Parameter(torch.zeros(1, 1 + addition, dim))
            trunc_normal_(self.pos_emb_5, std = 0.02)

        have_cls_token = False
        if pool == 'cls':
            have_cls_token = True
            self.cls_token_20 = nn.Parameter(torch.randn(1, 1, dim))
            self.cls_token_10 = nn.Parameter(torch.randn(1, 1, dim))
            self.cls_token_5 = nn.Parameter(torch.randn(1, 1, dim))
            trunc_normal_(self.cls_token_20, std=0.02)
            trunc_normal_(self.cls_token_10, std=0.02)
            trunc_normal_(self.cls_token_5, std=0.02)


        self.dropout = nn.Dropout(emb_dropout)

        assert len(depths) == 5
        self.transformer_20 = Transformer(dim, depths[0], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, 4,
                                          have_cls_token)
        self.transformer_20_to_10 = Transformer(dim, depths[1], heads, dim_head,
                                               mlp_dim, dropout, 'sa')
        self.transformer_10 = Transformer(dim, depths[2], heads, dim_head, mlp_dim, dropout,
                                         attn_type, shared_pe, 2,
                                         have_cls_token)
        self.transformer_10_to_5 = Transformer(dim, depths[3], heads, dim_head,
                                               mlp_dim, dropout, 'sa')
        self.transformer_5 = Transformer(dim, depths[4], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, 1,
                                          have_cls_token)
        

        if embed_weights == None:
            print('learnable embedding weights')
            self.learned_weights = nn.Parameter(torch.Tensor(3,1))
            nn.init.kaiming_uniform(self.learned_weights,a=math.sqrt(5))
        self.ms_dropout = nn.Dropout(dropout)



    def forward(self, x):
        b, _, _ = x.shape
        
        
        x_20 = x[:, :16, :]   # b 16 c
        x_10 = x[:, 16:20, :]  # b 4 c
        x_5 = x[:,20:,:]    # b 1 c 
        
        if self.patch_dim != self.dim:
            x_20 = self.to_patch_embedding_20(x_20)
            x_10 = self.to_patch_embedding_10(x_10)
            x_5 = self.to_patch_embedding_5(x_5)

        if self.pool == 'cls':

            cls_token_20 = repeat(self.cls_token_20, '() n d -> b n d', b=b)  #b 1 c
            x_20 = torch.cat((cls_token_20, x_20), dim=1)   #b 17 c
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 17 c
            x_20_cls_token = x_20[:,0,:]    # b c
            x_20 = x_20[:,1:,:]     # b 16 c

            x_20 = rearrange(x_20, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=2, h2=2, w1=2, w2=2)    # 4b 4 c
            x_10 = rearrange(x_10, 'b (n m) c -> (b n) m c', m=1)  # 4b 1 c
            x_20_10 = torch.cat((x_10, x_20), dim=1)    # 4b 5 c
            x_20_10 = self.transformer_20_to_10(x_20_10) # 4b 5 c
            x_10 = x_20_10[:, 0:1, :]       # 4b 1 c

            cls_token_10 = repeat(self.cls_token_10, '() n d -> b n d', b=b)  #b 1 c
            x_10 = rearrange(x_10, '(b n) m c -> b (n m) c', b=b)   #b 4 c
            x_10 = torch.cat((cls_token_10, x_10), dim=1)   #b 5 c
            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 5 c
            x_10_cls_token = x_10[:,0,:]      # b c
            x_10 = x_10[:,1:,:]       # b 4 c

            x_10 = rearrange(x_10, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                         h1=1, h2=2, w1=1, w2=2)    # b 4 c
            x_5 = rearrange(x_5, 'b (n m) c -> (b n) m c', m=1)  # b 1 c
            x_10_5 = torch.cat((x_5, x_10), dim=1)    # b 5 c
            x_10_5 = self.transformer_10_to_5(x_10_5) # b 5 c
            x_5 = x_10_5[:, 0:1, :]       # b 1 c

            cls_token_5 = repeat(self.cls_token_5, '() n d -> b n d', b=b)
            x_5 = rearrange(x_5, '(b n) m c -> b (n m) c', b=b)   #b 1 c
            x_5 = torch.cat((cls_token_5, x_5), dim=1)   # b 2 c
            if self.ape:
                x_5 += self.pos_emb_5
            x_5 = self.dropout(x_5)   
            x_5 = self.transformer_5(x_5)    # b 2 c
            x_5_cls_token = x_5[:,0,:]      # b c

        elif self.pool == 'mean':
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 16 c
            x_20_cls_token = x_20.mean(dim=1)      # b c

            x_20 = rearrange(x_20, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=2, h2=2, w1=2, w2=2)    # 4b 4 c
            x_10 = rearrange(x_10, 'b (n m) c -> (b n) m c', m=1)  # 4b 1 c
            x_20_10 = torch.cat((x_10, x_20), dim=1)    # 4b 5 c
            x_20_10 = self.transformer_10_to_10(x_20_10) # 4b 5 c
            x_10 = x_20_10[:, 0:1, :]       # 4b 1 c

            x_10 = rearrange(x_10, '(b n) m c -> b (n m) c', b=b)   #b 4 c
            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 4 c
            x_10_cls_token = x_10.mean(dim=1)      # b c
        
            x_10 = rearrange(x_10, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                         h1=1, h2=2, w1=1, w2=2)    # b 4 c
            x_5 = rearrange(x_5, 'b (n m) c -> (b n) m c', m=1)  # b 1 c
            x_10_5 = torch.cat((x_5, x_10), dim=1)    # b 5 c
            x_10_5 = self.transformer_10_to_5(x_10_5) # b 5 c
            x_5 = x_10_5[:, 0:1, :]       # b 1 c

            x_5 = rearrange(x_5, '(b n) m c -> b (n m) c', b=b)   #b 1 c
            if self.ape:
                x_5 += self.pos_emb_5
            x_5 = self.dropout(x_5)   
            x_5 = self.transformer_5(x_5)   # b 1 c
            x_5_cls_token = x_5.mean(dim=1)      # b c

        if self.embed_weights == None:

            learned_weights = torch.softmax(self.learned_weights,dim=0)

            x = learned_weights[0]*x_5_cls_token + learned_weights[1]*x_10_cls_token + learned_weights[2]*x_20_cls_token


       
        else:
            x_stack = torch.stack((self.embed_weights[0]*x_5_cls_token, 
                                   self.embed_weights[1]*x_10_cls_token, 
                                   self.embed_weights[2]*x_20_cls_token))
            x = torch.sum(x_stack,dim=0) # b c

        
        return x


'''
PyramidViT_2:
    Input ROI has size of 512 (at 20x). The number of tokens (patches) at 20x is 4. The size of each patch is 256x256.
    args:
        embed_weights: weight coefficient of instance embedding at each magnificant level (20x,10x), no 5x because number of patches at 10x is already 1.
            List with the length of 2 or 'None' (learnable weights).
        depths: depth of transformer block at each scale.
            List with the length of 3:
                [intra-scale SA at 20x, 
                inter-scale between 20x~10x,
                intra-scale SA at 10x]
        ape: whether to use positional encoding
'''
class PyramidViT_2(nn.Module):
    def __init__(self, num_patches, embed_weights, patch_dim, dim, depths, heads=4,
                 mlp_dim=512, pool='cls', dim_head = 64, dropout = 0., emb_dropout = 0.,
                 ape=True, attn_type='rel_sa', shared_pe=True):
        super().__init__()

        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        self.pool = pool

        self.patch_dim = patch_dim #dim of features extracted from ResNet
        self.dim = dim
        self.ape = ape
        self.embed_weights = embed_weights
        self.to_patch_embedding_20 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )
        self.to_patch_embedding_10 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )


        if ape:
            addition = 1 if pool == 'cls' else 0        #### pos embedding for cls_token
            self.pos_emb_20 = nn.Parameter(torch.zeros(1, 4+addition, dim))
            trunc_normal_(self.pos_emb_20, std=0.02)
            self.pos_emb_10 = nn.Parameter(torch.zeros(1, 1+addition, dim))
            trunc_normal_(self.pos_emb_10, std=0.02)


        have_cls_token = False
        if pool == 'cls':
            have_cls_token = True
            self.cls_token_20 = nn.Parameter(torch.randn(1, 1, dim))
            self.cls_token_10 = nn.Parameter(torch.randn(1, 1, dim))            
            trunc_normal_(self.cls_token_20, std=0.02)
            trunc_normal_(self.cls_token_10, std=0.02)



        self.dropout = nn.Dropout(emb_dropout)

        assert len(depths) == 3
        self.transformer_20 = Transformer(dim, depths[0], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, 2,
                                          have_cls_token)
        self.transformer_20_to_10 = Transformer(dim, depths[1], heads, dim_head,
                                               mlp_dim, dropout, 'sa')
        self.transformer_10 = Transformer(dim, depths[2], heads, dim_head, mlp_dim, dropout,
                                         attn_type, shared_pe, 1,
                                         have_cls_token)
        

        if embed_weights == None:
            print('learnable embedding weights')
            self.learned_weights = nn.Parameter(torch.Tensor(2,1))
            nn.init.kaiming_uniform(self.learned_weights,a=math.sqrt(5))
        self.ms_dropout = nn.Dropout(dropout)



    def forward(self, x):
        b, _, _ = x.shape
        
        
        x_20 = x[:, :4, :]   # b 4 c
        x_10 = x[:, 4:, :]  # b 1 c

        
        if self.patch_dim != self.dim:
            x_20 = self.to_patch_embedding_20(x_20)
            x_10 = self.to_patch_embedding_10(x_10)

        if self.pool == 'cls':

            cls_token_20 = repeat(self.cls_token_20, '() n d -> b n d', b=b)  #b 1 c
            x_20 = torch.cat((cls_token_20, x_20), dim=1)   #b 4 c
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 5 c
            x_20_cls_token = x_20[:,0,:]    # b c
            x_20 = x_20[:,1:,:]     # b 4 c

            x_20 = rearrange(x_20, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=1, h2=2, w1=1, w2=2)    # b 4 c
            x_10 = rearrange(x_10, 'b (n m) c -> (b n) m c', m=1)  # b 1 c
            x_20_10 = torch.cat((x_10, x_20), dim=1)    # b 5 c
            x_20_10 = self.transformer_20_to_10(x_20_10) # b 5 c
            x_10 = x_20_10[:, 0:1, :]       # b 1 c

            cls_token_10 = repeat(self.cls_token_10, '() n d -> b n d', b=b)  #b 1 c
            x_10 = rearrange(x_10, '(b n) m c -> b (n m) c', b=b)   #b 1 c
            x_10 = torch.cat((cls_token_10, x_10), dim=1)   #b 2 c
            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 2 c
            x_10_cls_token = x_10[:,0,:]      # b c

        elif self.pool == 'mean':
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 4 c
            x_20_cls_token = x_20.mean(dim=1)      # b c

            x_20 = rearrange(x_20, 'b (h1 h2 w1 w2) c -> (b h1 w1) (h2 w2) c',
                             h1=2, h2=2, w1=2, w2=2)    # b 4 c
            x_10 = rearrange(x_10, 'b (n m) c -> (b n) m c', m=1)  # b 1 c
            x_20_10 = torch.cat((x_10, x_20), dim=1)    # b 5 c
            x_20_10 = self.transformer_10_to_10(x_20_10) # b 5 c
            x_10 = x_20_10[:, 0:1, :]       # b 1 c

            x_10 = rearrange(x_10, '(b n) m c -> b (n m) c', b=b)   #b 4 c
            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 1 c
            x_10_cls_token = x_10.mean(dim=1)      # b c
        

        if self.embed_weights == None:
            learned_weights = torch.softmax(self.learned_weights,dim=0)

            x = learned_weights[0]*x_10_cls_token + learned_weights[1]*x_20_cls_token
            #print('x', x.shape)

       
        else:
            x_stack = torch.stack((self.embed_weights[0]*x_10_cls_token, 
                                   self.embed_weights[1]*x_20_cls_token))
            x = torch.sum(x_stack,dim=0) # b c

        
        return x


'''
PyramidViT_SingleScale:
    Single-scale model
    args:
        embed_weights: useless
        depths: depth of transformer block at each scale.
        ape: whether to use positional encoding
'''
class PyramidViT_SingleScale(nn.Module):
    def __init__(self, num_patches, embed_weights, patch_dim, dim, depths, level=0, heads=4,
                 mlp_dim=512, pool='cls', dim_head = 64, dropout = 0., emb_dropout = 0.,
                 ape=True, attn_type='rel_sa', shared_pe=True):
        super().__init__()

        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        self.pool = pool

        self.patch_dim = patch_dim #dim of features extracted from ResNet
        self.dim = dim
        self.ape = ape
        self.level = level
        self.embed_weights = embed_weights

        self.to_patch_embedding = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )
        self.patch_idx = [0,64,80,84] 


        patch_levels = [64,16,4]
        patch_length = [8,4,2]
        

        if ape:
            addition = 1 if pool == 'cls' else 0        #### pos embedding for cls_token
            self.pos_emb = nn.Parameter(torch.zeros(1, patch_levels[self.level]+addition, dim))
            trunc_normal_(self.pos_emb, std=0.02)

        have_cls_token = False
        if pool == 'cls':
            have_cls_token = True
            self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
            trunc_normal_(self.cls_token, std=0.02)


        self.dropout = nn.Dropout(emb_dropout)

        assert len(depths) == 5
        self.transformer = Transformer(dim, depths[0], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, patch_length[self.level],
                                          have_cls_token)

        self.ms_dropout = nn.Dropout(dropout)



    def forward(self, x):
        b, _, _ = x.shape
        
        
        x_level = x[:, self.patch_idx[self.level]:self.patch_idx[self.level+1], :]     # b n c
        
        if self.patch_dim != self.dim:
            x_level = self.to_patch_embedding(x_level)

        if self.pool == 'cls':
            cls_token = repeat(self.cls_token, '() n d -> b n d', b=b)
            x_level = torch.cat((cls_token, x_level), dim=1)
            if self.ape:
                x_level += self.pos_emb
            x_level = self.dropout(x_level)   
            x_level = self.transformer(x_level)    # b 65 c
            x_level_cls_token = x_level[:,0,:]      # b c

        elif self.pool == 'mean':
            if self.ape:
                x_level += self.pos_emb
            x_level = self.dropout(x_level)   
            x_level = self.transformer(x_level)    # b 64 c
            x_level_cls_token = x_level.mean(dim=1)      # b c

        

        x_level = torch.cat((x_level_cls_token,), dim=1)
        
        return x_level


'''
PyramidViT_wo_interscale:
    There are no inter-scale self-attention modules.
    args:
        embed_weights: weight coefficient of instance embedding at each magnificant level (20x,10x,5x)
            List with the length of 3 or 'None' (learnable weights).
        depths: depth of transformer block at each scale.
            List with the length of 3:
                [intra-scale SA at 20x, 
                intra-scale SA at 10x, 
                intra-scale SA at 5x]
        ape: whether to use positional encoding
'''
class PyramidViT_wo_interscale(nn.Module):
    def __init__(self, num_patches, embed_weights, patch_dim, dim, depths, heads=4,
                 mlp_dim=512, pool='cls', dim_head = 64, dropout = 0., emb_dropout = 0.,
                 ape=True, attn_type='rel_sa', shared_pe=True):
        super().__init__()

        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        self.pool = pool
        self.ape = ape
        
        self.patch_dim = patch_dim #dim of features extracted from ResNet
        self.dim = dim
        self.embed_weights = embed_weights
        self.to_patch_embedding_20 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )
        self.to_patch_embedding_10 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )
        self.to_patch_embedding_5 = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )

        if ape:
            addition = 1 if pool == 'cls' else 0        #### pos embedding for cls_token
            self.pos_emb_20 = nn.Parameter(torch.zeros(1, 64+addition, dim))
            trunc_normal_(self.pos_emb_20, std=0.02)
            self.pos_emb_10 = nn.Parameter(torch.zeros(1, 16+addition, dim))
            trunc_normal_(self.pos_emb_10, std=0.02)
            self.pos_emb_5 = nn.Parameter(torch.zeros(1, 4+addition, dim))
            trunc_normal_(self.pos_emb_5, std=0.02)
        
        have_cls_token = False
        if pool == 'cls':
            have_cls_token = True
            self.cls_token_20 = nn.Parameter(torch.randn(1, 1, dim))
            self.cls_token_10 = nn.Parameter(torch.randn(1, 1, dim))
            self.cls_token_5 = nn.Parameter(torch.randn(1, 1, dim))
            trunc_normal_(self.cls_token_20, std=0.02)
            trunc_normal_(self.cls_token_10, std=0.02)
            trunc_normal_(self.cls_token_5, std=0.02)
        
        self.dropout = nn.Dropout(emb_dropout)

        assert len(depths) == 3
        self.transformer_20 = Transformer(dim, depths[0], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, 8,
                                          have_cls_token)
        # self.transformer_20_to_10 = Transformer(dim, depths[1], heads, dim_head,
        #                                         mlp_dim, dropout, 'sa')     # no need to set have_cls_token
        self.transformer_10 = Transformer(dim, depths[1], heads, dim_head, mlp_dim, dropout,
                                          attn_type, shared_pe, 4,
                                          have_cls_token)
        # self.transformer_10_to_5 = Transformer(dim, depths[3], heads, dim_head,
        #                                        mlp_dim, dropout, 'sa')
        self.transformer_5 = Transformer(dim, depths[2], heads, dim_head, mlp_dim, dropout,
                                         attn_type, shared_pe, 2,
                                         have_cls_token)


        if embed_weights == None:
            print('learnable embedding weights')
            #'''
            self.ms_attn = nn.Sequential(
                mlpmixer(dim=dim, num_patches=3,
                         dim_expansion_factor=2, dropout=dropout),
                nn.Linear(dim, dim//2),
                nn.Tanh(),
                nn.Linear(dim//2, 3)
            )

        self.ms_dropout = nn.Dropout(dropout)

    def forward(self, x):
        b, _, _ = x.shape
        

        
        x_20 = x[:, :64, :]     # b 64 c
        x_10 = x[:, 64:80, :]   # b 16 c
        x_5 = x[:, 80:84, :]    # b 4 c
        
        if self.patch_dim != self.dim:
            x_20 = self.to_patch_embedding_20(x_20)
            x_10 = self.to_patch_embedding_10(x_10)
            x_5 = self.to_patch_embedding_5(x_5)

        if self.pool == 'cls':
            cls_token_20 = repeat(self.cls_token_20, '() n d -> b n d', b=b) #b 1 c
            x_20 = torch.cat((cls_token_20, x_20), dim=1)   # b 65 c
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 65 c
            x_20_cls_token = x_20[:,0,:]      # b c
            x_20 = x_20[:, 1:, :]       # b 64 c
            

            cls_token_10 = repeat(self.cls_token_10, '() n d -> b n d', b=b)
            x_10 = rearrange(x_10, '(b n) m c -> b (n m) c', b=b)   #b 16 c
            x_10 = torch.cat((cls_token_10, x_10), dim=1)   # b 17 c
            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 17 c
            x_10_cls_token = x_10[:,0,:]      # b c
            x_10 = x_10[:, 1:, :]       # b 16 c
            

            cls_token_5 = repeat(self.cls_token_5, '() n d -> b n d', b=b)
            x_5 = rearrange(x_5, '(b n) m c -> b (n m) c', b=b)   #b 4 c
            x_5 = torch.cat((cls_token_5, x_5), dim=1)   # b 5 c
            if self.ape:
                x_5 += self.pos_emb_5
            x_5 = self.dropout(x_5)   
            x_5 = self.transformer_5(x_5)    # b 5 c
            x_5_cls_token = x_5[:,0,:]      # b c
        
        elif self.pool == 'mean':
            if self.ape:
                x_20 += self.pos_emb_20
            x_20 = self.dropout(x_20)   
            x_20 = self.transformer_20(x_20)    # b 65 c
            x_20_cls_token = x_20.mean(dim=1)      # b c
            

            if self.ape:
                x_10 += self.pos_emb_10
            x_10 = self.dropout(x_10)   
            x_10 = self.transformer_10(x_10)    # b 17 c
            x_10_cls_token = x_10.mean(dim=1)      # b c
            

            if self.ape:
                x_5 += self.pos_emb_5
            x_5 = self.dropout(x_5)   
            x_5 = self.transformer_5(x_5)    # b 5 c
            x_5_cls_token = x_5.mean(dim=1)      # b c
       

        if self.embed_weights == None:
            x_stack = torch.stack([x_20_cls_token, x_10_cls_token,
                                   x_5_cls_token], dim=1)       ## b 3 c
            w = self.ms_attn(x_stack)       ## b 3
            w = torch.softmax(w, dim=1)
            x = torch.einsum('b m, b m c -> b c', w, x_stack)

        else:
            x_stack = torch.stack((self.embed_weights[0]*x_5_cls_token, 
                                   self.embed_weights[1]*x_10_cls_token, 
                                   self.embed_weights[2]*x_20_cls_token))
            x = torch.sum(x_stack,dim=0) # b c

        return x




class deepmil(nn.Module):
    def __init__(self, input_dim, hidden_dim=512, dropout=0.):
        super(deepmil, self).__init__()

        self.dim_trans = nn.Sequential(
            #nn.Linear(512*64, dim),
            nn.Linear(input_dim, hidden_dim*2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim*2, hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        self.attention = nn.Sequential(
            nn.Linear(input_dim, input_dim//2),
            nn.Tanh(),
            nn.Linear(input_dim//2, 1)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.dim_trans(x)       #b n 512
        a = self.attention(x)       #b n 1
        a = F.softmax(a, dim=1)
        
        x = einsum('b n d, b n a -> b a d', x, a).squeeze(1) #b,512
        x = self.dropout(x)

        return x


class mlpmixer(nn.Module):
    def __init__(self, dim=512, num_patches=20, dim_expansion_factor=2,
                 dropout=0.):
        super(mlpmixer, self).__init__()
        
        self.token_mixer = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Conv1d(num_patches, num_patches*dim_expansion_factor, 1),
            nn.GELU(),
            nn.Dropout(dropout),
            #nn.Dropout(0.25),
            nn.Conv1d(num_patches*dim_expansion_factor, num_patches, 1),
            #nn.Dropout(dropout)
        )

        self.channel_mixer = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, dim*dim_expansion_factor),
            nn.GELU(),
            nn.Dropout(dropout),
            #nn.Dropout(0.25),
            nn.Linear(dim*dim_expansion_factor, dim),
            #nn.Dropout(dropout)
        )

    def forward(self, x):
        idn1 = x
        x = self.token_mixer(x)
        x = x + idn1
        idn2 = x
        x = self.channel_mixer(x)
        x = x + idn2

        return torch.mean(x, dim=1)

'''
Class-specific gated attention net of CLAM. For instance aggregation.
args:
    input_dim: input feature dimension
    out_dim: out feature dimension
    n_classes: number of classes
'''
# modified by CLAM
class Attention_net_gated(nn.Module):
    def __init__(self,input_dim = 256,out_dim = 256,n_classes = 1,dropout=0):
        super(Attention_net_gated,self).__init__()
        self.attention_a = [nn.LayerNorm(input_dim),nn.Linear(input_dim,out_dim),nn.Tanh()]
        self.attention_b = [nn.LayerNorm(input_dim),nn.Linear(input_dim,out_dim),nn.Sigmoid()]

        if dropout>0:
            self.attention_a.append(nn.Dropout(dropout))
            self.attention_b.append(nn.Dropout(dropout))
            #self.attention_a.append(nn.Dropout(0.2))
            #self.attention_b.append(nn.Dropout(0.2))
        
        self.attention_a = nn.Sequential(*self.attention_a)
        self.attention_b = nn.Sequential(*self.attention_b)
        
        self.attention_c = nn.Linear(out_dim, n_classes)
    
    def forward(self,x):
        a = self.attention_a(x)
        b = self.attention_b(x)
        A = a.mul(b)
        A = self.attention_c(A)
        return A,x


'''
Class-specific normal attention net of CLAM. For instance aggregation.
args:
    input_dim: input feature dimension
    out_dim: out feature dimension
    n_classes: number of classes
'''
class Attention_net(nn.Module):
    def __init__(self,input_dim = 256,out_dim = 256,n_classes = 1,dropout=0):
        super(Attention_net,self).__init__()
        self.attention_a = [nn.LayerNorm(input_dim), nn.Linear(input_dim,out_dim), nn.Tanh()]

        if dropout>0:
            self.attention_a.append(nn.Dropout(dropout))
            #self.attention_a.append(nn.Dropout(0.2))
        
        self.attention_a = nn.Sequential(*self.attention_a)
        
        self.attention_c = nn.Linear(out_dim, n_classes)
    
    def forward(self,x):
        a = self.attention_a(x)
        A = self.attention_c(a)
        return A,x


class Classifier(nn.Module):
    def __init__(self, dim, n_classes):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fc = nn.Linear(dim, n_classes)

    def forward(self, x):
        x = self.norm(x)
        x = self.fc(x)
        return x

'''
ROAM: ROAM model main architecture
args:
    choose_num: the number of instances (topk) used in instance-level supervision
    num_patches: the number of patches (tokens)
    patch_dim: input patch dimension
    num_classes: the number of classes
    roi_level: size of ROI at 20x, (0:2048,1:1024,2:512)
    scale_type: 'ms' (multi-scale) or 'ss' (single scale)
    embed_weights: weight coefficient of instance embedding at each magnificant level (5x,10x,20x)
    dim: input feature dimension
    depths: depths (number of Transformer layers) of the Transformer block
    heads: number of heads
    mlp_dim: dimension of hidden layer feature
    not_interscale: False: there exists inter-scale self-attention module, True: there is no inter-scale self-attention module
    single_level: magnification scale of input ROI for single-scale model. (0:20x,1:10x,2:5x)
    dim_head: feature dimension of each head
    dropout: dropout of self-attentio module and feedforward module of transformer
    emb_dropout: dropout of multi-scale self-attention network
    attn_dropout: dropout of attention network in instance aggregation module
    pool: whether to use cls token. 'cls': use cls token. 'mean': no cls token
    ape: whether to use positional encoding
    attn_type: the type of self-attention layer ('sa': normal self-attention, 'rel_sa': relative self-attention)
    shared_pe: whether to share relative position embedding across all the heads
'''
class ROAM(nn.Module):
    def __init__(self, *, choose_num, num_patches, patch_dim, 
                 num_classes, roi_level, scale_type,
                 embed_weights, dim, depths, heads, mlp_dim, 
                 not_interscale=False, single_level=0,
                 dim_head=64, dropout=0., emb_dropout=0., attn_dropout=0., 
                 pool='cls', ape=True, attn_type='rel_sa', shared_pe=True):
        super().__init__()
        self.topk = choose_num
        self.num_classes = num_classes
        self.roi_level = roi_level
        self.scale_type = scale_type
        self.embed_weights = embed_weights

        if self.scale_type == 'ms':
            if not_interscale:
                self.vit = PyramidViT_wo_interscale(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
            else:
                if self.roi_level == 0:
                    self.vit = PyramidViT_0(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
                elif self.roi_level == 1:
                    self.vit = PyramidViT_1(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
                else:
                    self.vit = PyramidViT_2(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
        else:
            self.vit = PyramidViT_SingleScale(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, level=single_level, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
        
        ## attention net
        self.att_net = Attention_net_gated(input_dim=dim,out_dim=dim//2,n_classes=num_classes,dropout=attn_dropout)
        
        self.roi_clf = Classifier(dim, num_classes)
        self.slide_clfs = nn.ModuleList([Classifier(dim, 1) for i in range(num_classes)])

        '''
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        '''

        #self.loss_fn = SmoothTop1SVM(n_classes=2).cuda()
        self.loss_fn = nn.CrossEntropyLoss()


                

    def forward(self,x,label=None,inst_level=False,grade=False):
        device = x.device
        b,k, _, _ = x.shape
        x = rearrange(x, 'b k n d -> (b k) n d')
        x = self.vit(x)         # (b*k dim) 
        x = rearrange(x, '(b k) d -> b k d', b=b)
        slide_logits = torch.empty(b, self.num_classes).float().to(device)
        inst_loss_all = 0.0
        for b_i in range(b):
            h = x[b_i] #n_rois,dim

            att_embed,_ = self.att_net(h) #k,n_classes
            att_embed = torch.transpose(att_embed,1,0) #n_classes,k
            att_embed = F.softmax(att_embed,dim=1) #n_classes,k

            total_inst_loss = 0.0
            tmp_topk = min(self.topk, k)
            ## instance-level supervision
            if inst_level:
                l = label[b_i]
                att_cur_cat = att_embed[l]
                in_top_p_idx = torch.topk(att_cur_cat, tmp_topk, dim=0)[1]
                in_top_p = h[in_top_p_idx]
                in_p_targets = torch.full((tmp_topk, ), l, device=device).long()

                logits = self.roi_clf(in_top_p)
                inst_loss = self.loss_fn(logits, in_p_targets)
                total_inst_loss += inst_loss
            

            h_weighted = torch.mm(att_embed,h) #n_classes,dim
            for c in range(self.num_classes):
                slide_logits[b_i,c] = self.slide_clfs[c](h_weighted[c])
            
            '''
            tmp_out = self.slide_clf(h_weighted)[0]
            #print(tmp_out.shape, slide_logits[b_i].shape)
            slide_logits[b_i] = tmp_out 
            '''
        return slide_logits,total_inst_loss


'''
ROAM_VIS:
    ROAM version for visualization, has the same architecture with ROAM model.
    Only modify input and output of __forward__ function for visualization
'''
class ROAM_VIS(nn.Module):
    def __init__(self, *, choose_num, num_patches, patch_dim, 
                 num_classes, roi_level, scale_type,
                 embed_weights, dim, depths, heads, mlp_dim, 
                 not_interscale=False, single_level=0,
                 dim_head=64, dropout=0., emb_dropout=0., attn_dropout=0., 
                 pool='cls', ape=True, attn_type='rel_sa', shared_pe=True):
        super().__init__()
        #self.aggr_method = aggr_method
        self.topk = choose_num
        self.num_classes = num_classes
        self.roi_level = roi_level
        self.scale_type = scale_type
        self.embed_weights = embed_weights

        if self.scale_type == 'ms':
            if not_interscale:
                self.vit = PyramidViT_wo_interscale(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
            else:
                if self.roi_level == 0:
                    self.vit = PyramidViT_0(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
                elif self.roi_level == 1:
                    self.vit = PyramidViT_1(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
                else:
                    self.vit = PyramidViT_2(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
        else:
            self.vit = PyramidViT_SingleScale(num_patches=num_patches, embed_weights = self.embed_weights,
                                    patch_dim=patch_dim, dim=dim,
                                    depths=depths, level=single_level, heads=heads, mlp_dim=mlp_dim, 
                                    pool=pool, dim_head=dim_head, dropout=dropout,
                                    emb_dropout=emb_dropout, ape=ape,
                                    attn_type=attn_type,
                                    shared_pe=shared_pe)
        
        ## attention net
        self.att_net = Attention_net_gated(input_dim=dim,out_dim=dim//2,n_classes=num_classes,dropout=attn_dropout)
        
        self.roi_clf = Classifier(dim, num_classes)
        self.slide_clfs = nn.ModuleList([Classifier(dim, 1) for i in range(num_classes)])

        '''
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        '''

        #self.loss_fn = SmoothTop1SVM(n_classes=2).cuda()
        self.loss_fn = nn.CrossEntropyLoss()


                
    '''
    args:
        vis: whether to use visualization mode
            False: normal mode, with output similar to that during ROAM training
                input: patch features extracted from pre-trained model
                output: slide-level prediciton and instance-level loss
        vis_mode: configure of visualization
            1: no instance aggregation is performed
                input: patch features extracted from pre-trained model
                output: instance-level representations
            2: 
                input: instance-level representations
                output: slide-level prediciton and attention scores of each ROI (instance)
            3:
                input: patch features extracted from pre-trained model
                output: slide-level prediciton and attention scores of each ROI (instance)
    '''
    def forward(self,x,label=None,inst_level=False,grade=False,vis=False,vis_mode=1):
        if not vis:
            device = x.device
            b,k, _, _ = x.shape
            x = rearrange(x, 'b k n d -> (b k) n d')
            x = self.vit(x)         # (b*k dim) 
            x = rearrange(x, '(b k) d -> b k d', b=b)
            slide_logits = torch.empty(b, self.num_classes).float().to(device)
            inst_loss_all = 0.0
            for b_i in range(b):
                h = x[b_i] #n_rois,dim
                #device = h.device

                att_embed,_ = self.att_net(h) #k,n_classes
                roi_attns = att_embed
                att_embed = torch.transpose(att_embed,1,0) #n_classes,k
                att_embed = F.softmax(att_embed,dim=1) #n_classes,k

                total_inst_loss = 0.0
                #inst_loss_list = []
                tmp_topk = min(self.topk, k)
                if inst_level:
                    l = label[b_i]
                    att_cur_cat = att_embed[l]
                    in_top_p_idx = torch.topk(att_cur_cat, tmp_topk, dim=0)[1]
                    in_top_p = h[in_top_p_idx]
                    in_p_targets = torch.full((tmp_topk, ), l, device=device).long()

                    logits = self.roi_clf(in_top_p)
                    inst_loss = self.loss_fn(logits, in_p_targets)
                    total_inst_loss += inst_loss
                

                h_weighted = torch.mm(att_embed,h) #n_classes,dim
                for c in range(self.num_classes):
                    slide_logits[b_i,c] = self.slide_clfs[c](h_weighted[c])
            if vis_mode == 3:
                return slide_logits,roi_attns
                
            return slide_logits,total_inst_loss
        else:
            if vis_mode==1:
                k, _, _ = x.shape
                x = self.vit(x)         # (n_rois_batch,dim) 
                return x
            if vis_mode==2:
                device = x.device
                # x --> n_rois_all.dim
                slide_logits = torch.empty(1, self.num_classes).float().to(device)
                h = x
                att_embed,_ = self.att_net(h) #k,n_classes

                roi_attns = att_embed
                att_embed = torch.transpose(att_embed,1,0) #n_classes,k
                att_embed = F.softmax(att_embed,dim=1) #n_classes,k

                h_weighted = torch.mm(att_embed,h) #n_classes,dim

                for c in range(self.num_classes):
                    slide_logits[0, c] = self.slide_clfs[c](h_weighted[c])
                
                return slide_logits,roi_attns


            



